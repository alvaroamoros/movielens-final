---
title: "Movielens Recomendation System"
author: "Álvaro Amorós Rodríguez"
date: "16/4/2021"
output:
  pdf_document: default
  html_document: default
subtitle: "HarvardX: PH125.9x Data Science: Capstone"
---
# 1.Introduction.

## 1.1 Summary.
The objective of this project is to put in to practice some of the techniques and skills learned during the HarvardX Data Science Professional
Certificate courses, and apply them to generate a movie recommendation system. To do so, I will use the movielens dataset provided by the course, which contains 9 million observations provided by 69878 users on 10677 movies. The data has 6 dimensions, userId, movieId, rating, tiemstamp, title, and genders. My is goal is to use this dimensions to predict the future ratings that viewers will give to movies. The course also provides a 1 million observations validation dataset in which I will test the validity of my final model. In the first part of the project, I will conduct descriptive and visual analysis of the data, and I will prepare it to tests or subsequent predictive models. In the second part, I will explore de advantages and limitations of different models, and apply them on the  *validation* dataset, to asses the overall validity of those. The third and final part will consist of a small summary of my conclusions.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r Load Data and Libraries, echo=FALSE, include=FALSE}
# Loading libraries and data. 
library(ggplot2)
library(scales)
library(ggthemes)
library(data.table)
library(reshape2)
library(tidyverse)
library(caret)
library(readr)
library(lubridate)
library(recosystem)
library(gridExtra )
library(dplyr, warn.conflicts = FALSE)

# Suppress summarise info
options(dplyr.summarise.inform = FALSE)

load("edx.Rda")
load("validation.Rda")
edx$X <- NULL
validation$X <- NULL



```


## 1.2 Data.
There are 69878 users in the dataset, which rated 10677 movies, with and average rating of 3.51 and a standard deviation of 1.06, being the highest rating 5 and the lowest 0.5, each movie has and average number of 5429 ratings and each user has rated and average number of 340 movies.

```{r Train and Test, echo=FALSE}
# Creating train and test sets
set.seed(1, sample.kind="Rounding")

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, 
                                  list = FALSE)

train_set <- edx[-test_index,]
test_set <- edx[test_index,]


test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")


```

```{r Data, echo=FALSE}
# Summary Statistics
train_set %>%
  group_by(movieId) %>%
  mutate(n_ratings_movie = n()) %>%
  ungroup() %>%
  group_by(userId) %>%
  mutate(n_ratin_user = n ()) %>%
  ungroup() %>%
  summarise(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId),
            n_ratings = nrow(edx),
            avg_rating = mean(as.numeric(rating)),
            sd_rating = sd(as.numeric(rating)),
            max_raing = max(rating),
            min_rating = min(rating),
            mean_n_ratings_movie = round(mean(n_ratings_movie)),
            mean_n_rating_user = round(mean(n_ratin_user))) %>% knitr::kable(caption = "Summary statistics",
                                                       col.names = c("Users", "Movies", "Ratings", "Avg.", "Sd",
                                                                     "Max", "Min", "Avg. N. movie", "Avg. N. user"),
                                                       align = "l")

```

```{r Variables, echo=FALSE}
# The data in a tibble
as_tibble(train_set) %>%
  head() %>% knitr::kable(caption = "Data table", align = "l")


```



# 2. Analysis

##2.1 Preparation
To test different models before applying them to the *validation* dataset, I will divide the data in two sets, *train set* and *test set*, with the first containing 90% of the observations and the second 10%.

Besides the information a about the rating that each user gave to each movie, we have data on when this rating was given and on the genre or genres of the each movie. One can expect a certain time effect in the data, with users giving higher ratings in specific periods and lower in others. Genre should be also a valid predictor for rating as certain users will have preferences towards certain genders. But given the size of the dataset, at the limited memory and computing power of my laptop, this last two dimensions will not be used. 

## 2.2 Visual analysis.
A preliminary visual inspection of the data can be useful to get a glance of the predictive capacity of different dimensions. From the five graphs generated we can extract a set of observations:

+ Higher ratings and prevalent.
+ The user average rating distributions is almost perfectly normal.
+ The movie average rating is skewed to the right.
+ The distribution of number of ratings per user is skewed to the left.
+ The distribution of number of ratings per movie is close to normal.

Regarding the users,on one hand, we can see that most of them have rated a small number of movies, while a small group of them have rated a significantly high number. On the other hand, the distribution of the mean rating of users is almost perfectly normal, with most of the mean ratings concentrating around the mean. If we look at the number of ratings per movie, we can see that some block busters have very high number of ratings while another group of movies has almost none. The average movie rating is skewed to the right, with only a few movies having a rating higher than 4. This information about the data will be useful as we start testing or different models.

```{r 1, echo=FALSE, fig.align='center', fig.width=8, fig.height=4}
# Rating distribution
rating_distribution <-
train_set %>% group_by(rating) %>% 
  summarise(count=n(), .groups = 'drop') %>%
  ggplot(aes(x=rating, y=count)) + 
  geom_bar(stat = "identity") +
  ggtitle("Rating Distribution") + 
  xlab("Rating") +
  ylab("Count") + 
  theme_hc()
rating_distribution 

```




```{r 2, echo=FALSE, fig.align='center', fig.width=5, fig.height=3.5}
# Number of votes per user
n_ratings_user <- 
train_set %>% 
  group_by(userId) %>%
  summarise(n=n(), .groups = 'drop') %>%
  ggplot(aes(x = n)) +
  geom_histogram(color = "black", bins = 30) +
  scale_x_log10() + 
  ggtitle("Number of ratings per User") +
  xlab("Number of Ratings") +
  ylab("Number of Users") + 
  scale_y_continuous(labels = comma) +
  theme_hc()

```

```{r 2.5, echo=FALSE, fig.align='center', fig.width=5, fig.height=3.5}
# User average rating
mean_user_vote <-
train_set %>%
  group_by(userId) %>%
  summarise(mean_vote = mean(rating), .groups = 'drop') %>%
  ggplot(aes(mean_vote)) +
  geom_histogram(bins = 30, color = "black") +
  ggtitle("User average rating") +
  xlab("User average rating") +
  ylab("Count") + 
  scale_y_continuous(labels = comma) +
  theme_hc()

```

```{r 3, echo=FALSE, fig.align='center', fig.width=5, fig.height=3.5}
# Number of votes per movie
n_ratings_movie <-
train_set %>% group_by(movieId) %>%
  summarise(n=n(), .groups = 'drop') %>%
  ggplot(aes(n)) +
  geom_histogram(color = "black", bins = 30) +
  scale_x_log10() + 
  ggtitle("Number of Rarings per movie") +
  xlab("Number of Ratings") +
  ylab("Count") +
  theme_hc()

```

```{r 3.5, echo=FALSE, fig.align='center', fig.width=5, fig.height=3.5}
# Average movie ratings
mean_movie_vote <-
train_set %>% group_by(movieId) %>%
  summarise(mean_vote = mean(rating), .groups = 'drop') %>%
  ggplot(aes(mean_vote)) +
  geom_histogram(bins = 30, color = "black") +
  ggtitle("Movie average rating") +
  xlab("Movie average rating") +
  ylab("Count") +
  theme_hc()

```
  
```{r 3.75, echo=FALSE, fig.align='center', fig.width=12, fig.height=6}
# Putting all four graphs together in a grid.
grid.arrange( n_ratings_user, n_ratings_movie, mean_user_vote, mean_movie_vote)
  
```


# 3 Results

## 3.1 Model testing
Before applying the final models to the *validation* dataset, I will test a set of models using the train and test datasets. I will start wit a set of simple linear models to conclude with a more complex collaborative filtering algorithm based on matrix factorization. To asses the validity of each model I will use the *Root-mean-square deviation*, which according to Wikipedia *"represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences."*

> RMSE = sqrt(mean((true_ratings - predicted_ratings)^2))

```{r RMSE, echo=FALSE}
# Defining the function for the RMSE
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

```

### 3.1.1 Linear Model

#### Naive model
The simplest model possible in order to minimize mean squared error is to use the mean rating of our sample to predict ratings. With this model we obtain a RMSE of 1.06, which is the same as the standard deviation. I will create a table in which to store the results as well as the objective RMSE given by the exercise (0.864900) in order to be able to better compare the different models.


```{r 5, echo=FALSE}
# Mean rating in the train set
mu <- mean(train_set$rating)

# Naive rmse
naive_rmse <- RMSE(test_set$rating, mu)

# Tibble to store results
rmse_results <- tibble(method = "Objective", RMSE = 0.864900)
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Mean",
                                 RMSE = naive_rmse))

rmse_results %>% knitr::kable(caption = "Models", align = "l")

```

#### Movie effect
As shown by the graphical analysis conducted in the previous part, different movies have different mean ratings, as this average ratings follow a specific distribution, we can use this to further improve our model. The simplest way to incorporate this to the model is to calculate the standard deviation from the mean of each movie individually, and take into account this deviation when predicting ratings. With this model we improve the RMSE to 0.943.

```{r 6, echo=FALSE, fig.align='center', fig.width=5, fig.height=3.5}
# The deviation of each movie from the mean rating
bi <- train_set %>%
  group_by(movieId, .groups = 'drop') %>%
  summarise(b_i = mean(rating - mu))


```


```{r 7, echo=FALSE, fig.align='left'}
# Add the movie effect to the model
y_hat_bi <- mu + test_set %>%
  left_join(bi, by = "movieId") %>%
  pull(b_i)

# Calculate RMSE
movie_rmse <- RMSE(test_set$rating, (y_hat_bi))

# Add RMSE to the tibble.
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Movie Effect",
                                 RMSE = movie_rmse))

rmse_results %>% knitr::kable(caption = "Models", align = "l")

```


#### User effect.
As it happens with the mean rating of movies, each specific user has its own mean rating, altogether, this average ratings form a normal distribution around the overall mean. To incorporate this to our model, we will incorporate the deviation of the average of each user to the overall mean (ones the movie effect has been already incorporated). By adding the user effect to the model, the RMSE improves to 0.864.

```{r 8, echo=FALSE, fig.align='center', fig.width=5, fig.height=3.5}
# The deviation of each users from the overall mean + movie effect.
bu <- train_set %>%
  left_join(bi, by = "movieId") %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu - b_i))

```

```{r 9, echo=FALSE}
# Adding the user effect to the model.
y_hat_bu <- test_set %>%
  left_join(bi, by = "movieId") %>%
  left_join(bu, by = "userId") %>%
  mutate(prediction = mu + b_i + b_u) %>%
  pull(prediction)

# Calculating RMSE
user_rmse <- RMSE(test_set$rating, y_hat_bu)

# Adding RMSE to the table.
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Movie and User Effects",
                                 RMSE = user_rmse))
rmse_results %>% knitr::kable(caption = "Models", align = "l")

```

#### Regilarizartion.   
The RMSE  has  reached the desired objective of 0.8649 by very little. We can analyze which are the predictions that have deviated more from the data, for that we can look at the margins of our users and movies distribution. As we can see in Tables 6 and 7, the movies and users that deviate more from the mean are those with a very low number of ratings, as a low number of observations increases the standard error. The top 10 movies that deviate more from our predictions have between 1 and 4 ratings, white the 10 users that deviate more have between 15 and 28 ratings. This numbers ar far away from the average number of ratings per movie and user, 6108 and 382 respectively.  This movies and users correspond to the tails of our distribution.  To minimize the effect those users and movies in our predictions, we can use a penalization term which will give those movies and users with a low number of ratings less weight in our model.

```{r 9.5, echo=FALSE, fig.align='center', fig.width=8, fig.height=3}
# Deviation of movies and users from the mean.

# Movie Deviations
movie_agvs <- train_set %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating - mu))

# User deviations
user_agvs <- train_set %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu))

# Graph 1
g_1 <- qplot(b_i, data = movie_agvs, bins = 30, color = I("black"), main = "Deviation of movies from the mean ", ylab = "N. Rarings", xlab = "Movie effect")

# Graph 2
g_2 <- qplot(b_u, data = user_agvs, bins = 30, color = I("black"), main = "Deviation of users from the mean", ylab = "N. Rarings", xlab = "User effect")

# Grid
grid.arrange(g_1, g_2, nrow = 1)

```

```{r 10, echo=FALSE}
# Table with movies and users with the greatest deviations and its rating

# Movies
movie_titles <- edx %>% 
  select(movieId, title, rating) %>%
  distinct(movieId, title, rating)

train_set %>% dplyr::count(movieId) %>% 
  left_join(bi) %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n, rating) %>% 
  slice(1:10) %>% 
  knitr::kable(caption = "Movies with the highest deviation from the mean",
               col.names = c("Title", "Deviation", "N. Votes", "Rating"),
               align = "l")

# Users
user_ids <- edx %>% 
  select(userId, title, rating) %>%
  distinct(userId, rating)

train_set %>% dplyr::count(userId) %>% 
  left_join(bu) %>%
  left_join(user_ids, by="userId") %>%
  arrange((b_u)) %>% 
  select(userId, b_u, n, rating) %>% 
  slice(1:10) %>% 
  knitr::kable(caption = "users with the highest deviation from the mean",
               col.names = c("User", "Deviation", "N. Votes", "Rating"),
               align = "l")
```
 
 
```{r 11, echo=FALSE, include=FALSE}
# Regularization of the movie effect.

# Different regularization terms we will test
lambdas_1 <- seq(0, 10, 0.5)

# Regularization function
rmses_1 <- sapply(lambdas_1, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu) / (n() + l))
  
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - mu - b_i) / (n()))
  
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(predictions = mu + b_i + b_u) %>%
    .$predictions
  
  return(RMSE(test_set$rating, predicted_ratings))
  
  })

l_1 <- lambdas_1[which.min(rmses_1)]

```


```{r 11.5, echo=FALSE, include=FALSE}
# Regularization of user effect

# Different regularization terms we will test
lambdas_2 <- seq(0, 10, 0.5)

# Regularization function
rmses_2 <- sapply(lambdas_2, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu) / (n() + l_1))
  
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - mu - b_i) / (n() + l ))
  
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(predictions = mu + b_i + b_u) %>%
    .$predictions
  
  return(RMSE(test_set$rating, predicted_ratings))
  
  })

# Choose lambada which minimizes rmse
l_2 <- lambdas_2[which.min(rmses_2)]



```
To find the optimal penalization term we will use an algorithm that will test different penalization between 0 and 10. The lowest RMSE is obtained with a penalization of 4.5 for the movie effect and 5 for user effect. The final linear model will include the movie effect, the user effect and the realizations Lambda outputs, this will yield a RMSE of 0.8641359, which meets the requirements of the exercise.


```{r 12, echo=FALSE, fig.align='center', fig.width=8, fig.height=3}
# Graphs of different RMSEs for for different  regularization terms 

regu1 <- qplot(lambdas_1, rmses_1, main = "Regularization Movie effect",
      xlab = "Lambda",
      ylab = "RMSE")  

regu2 <- qplot(lambdas_2, rmses_2, main = "Regularization User effect",
      xlab = "Lambda",
      ylab = "RMSE")  

grid.arrange(regu1, regu2, nrow = 1)


```
  


```{r 13, echo=FALSE, include=FALSE}
# Add lambdas which minimize rmse to the model which user and movie effects
b_i <- train_set %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu) / (n() + l_1))
  
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - mu - b_i) / (n() + l_2))
  
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(predictions = mu + b_i + b_u) %>%
    .$predictions
  
regularization_model <-  RMSE(test_set$rating, predicted_ratings)

```

```{r 14, echo=FALSE}
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Movie + User Effects + Regularization",
                                 RMSE = regularization_model))
rmse_results %>% knitr::kable(caption = "Models", align = "l")

```


### 3.1.2 Matrix factorization. 
Matrix factorization is a class of collaborative filtering algorithm used to build recommendation systems. Recommendation systems based on collaborative filtering rely on the assumption that users with similar tastes will rate the same items in similar ways. Hence, the missing ratings of user *A* can be inferred from other similar users, which are refer to as a *"neighborhood"*. This rating patters of users are also present in between items. With matrix factorization one can extract this "hidden" structure of the data and use it to make predictions. To do so, in this project we will use the *recosystem* package.



```{r 15, echo=FALSE}
# Prepare the data to be used in with recosystem
train_data <-  with(train_set, data_memory(user_index = userId, 
                                           item_index = movieId, 
                                           rating     = rating))
test_data  <-  with(test_set,  data_memory(user_index = userId, 
                                           item_index = movieId, 
                                           rating     = rating))

# Create a Recommender object
r <- recosystem::Reco()


```


Ones our data has been transformed to use with the recommended package, we can use the *tune* function to fit the best parameters: number of latent factors, gradient descent rate and penalty parameter to avoid overfiting. Ones the best parameters have been defined we can train the mode. With the collaborative filtering model we obtain a significantly imported RMSE that meets the objective of the exercise.

```{r 16, echo=FALSE}
# create and object to save the the parameters that fit the best: number of latent factors, gradient descent rate and penalty parameter to avoid overfiting.
opts <- r$tune(train_data, opts = list(dim = c(10, 20, 30), 
                                       lrate = c(0.1, 0.2),
                                       costp_l2 = c(0.01, 0.1), 
                                       costq_l2 = c(0.01, 0.1),
                                       nthread  = 4, niter = 10))

```

```{r 17, echo=FALSE}
# Train the data with the parameters that maximize the fit of the model
r$train(train_data, opts = c(opts$min, nthread = 4, niter = 20))

# Predict outcomes
predicted_ratings <-  r$predict(test_data, out_memory())

# Test RMSE
factorization_model <- RMSE(test_set$rating, predicted_ratings)
factorization_model
```

```{r 18, echo=FALSE}
# Add results to table with the other models
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Matrix Factorization",
                                 RMSE = factorization_model))
rmse_results %>% knitr::kable(caption = "Models", align = "l")
```

## 3.2 Final model and results.

To asses the final validity of our models we will test them with the *edx* and *validation* datasets. For our linear model with regularization we obtain a RMSE of 0.8648177, which meets our required 0.8649, while for the collaborative filtering algorithm we obtain significant improvements with a RMSE fo 0.7824548

```{r 19, echo=FALSE}
# Naive
mu <- mean(edx$rating)
naive_rmse <- RMSE(validation$rating, mu)
rmse_results <- tibble(method = "Objective", RMSE = 0.864900)


# Movie effect
bi <- edx %>%
  group_by(movieId, .groups = 'drop') %>%
  summarise(b_i = mean(rating - mu))

y_hat_bi <- mu + validation %>%
  left_join(bi, by = "movieId") %>%
  pull(b_i)

movie_rmse <- RMSE(validation$rating, y_hat_bi)



# User effect
bu <- edx %>%
  left_join(bi, by = "movieId") %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu - b_i))

y_hat_bu <- validation %>%
  left_join(bi, by = "movieId") %>%
  left_join(bu, by = "userId") %>%
  mutate(prediction = mu + b_i + b_u) %>%
  pull(prediction)

user_rmse <- RMSE(validation$rating, y_hat_bu)


# Regularization
lambdas_1 <- seq(0, 10, 0.5)

rmses_1 <- sapply(lambdas_1, function(l){
  
  mu <- mean(edx$rating)
  
  b_i <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu) / (n() + l))
  
  b_u <- edx %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - mu - b_i) / (n()))
  
  predicted_ratings <- validation %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(predictions = mu + b_i + b_u) %>%
    .$predictions
  
  return(RMSE(validation$rating, predicted_ratings))
  
  })

l_1 <- lambdas_1[which.min(rmses_1)]


lambdas_2 <- seq(0, 10, 0.5)

rmses_2 <- sapply(lambdas_2, function(l){
  
  mu <- mean(edx$rating)
  
  b_i <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu) / (n() + l_1))
  
  b_u <- edx %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - mu - b_i) / (n() + l ))
  
  predicted_ratings <- validation %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(predictions = mu + b_i + b_u) %>%
    .$predictions
  
  return(RMSE(validation$rating, predicted_ratings))
  
  })
l_2 <- lambdas_2[which.min(rmses_2)]


b_i <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu) / (n() + l_1))
  
  b_u <- edx %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - mu - b_i) / (n() + l_2))
  
  predicted_ratings <- validation %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(predictions = mu + b_i + b_u) %>%
    .$predictions
  
regularization_model <-  RMSE(validation$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Movie + User  + Regularization (edx-validation)",
                                 RMSE = regularization_model))


# Matrix fatorization 

train_data_edx <-  with(edx, data_memory(user_index = userId, 
                                           item_index = movieId, 
                                           rating     = rating))
test_data_validation  <-  with(validation,  data_memory(user_index = userId, 
                                           item_index = movieId, 
                                           rating     = rating))

# Create a Recommender object
r <- recosystem::Reco()


# Optimize parametes
opts <- r$tune(train_data_edx, opts = list(dim = c(10, 20, 30), 
                                       lrate = c(0.1, 0.2),
                                       costp_l2 = c(0.01, 0.1), 
                                       costq_l2 = c(0.01, 0.1),
                                       nthread  = 4, niter = 10))

# Run model
r$train(train_data_edx, opts = c(opts$min, nthread = 4, niter = 20))

predicted_ratings <-  r$predict(test_data_validation, out_memory())

factorization_model <- RMSE(validation$rating, predicted_ratings)
factorization_model

# Final results table
rmse_results <- bind_rows(rmse_results,
                          tibble(method = "Matrix Factorization (edx-validation)",
                                 RMSE = factorization_model))
rmse_results %>% knitr::kable(caption = "Models", align = "l")





```

# 4. Conclusions.
The collaborative filtering model has proven to be by far the best model to predict ratings, and supposes a significant increase in the RMSE when compared to linear models, besides that, the model only needs a feedback matrix to be trained as it does not require any type contextual features which can be useful in situations where we lack labeled information. A last advantage of this approach is that it can be used to discover new interests of the users and hidden structures in the data (although this is not done in this project.)Regarding the limitations, the most important one is the incapacity of the model to rate new items, as if the item was not present in the train set, it will not be able to generate any prediction. The second limitation comes from the incapacity of the model to use secondary information besides its Id and rating, in the case of this dataset, other type of model could use genre or timestamp as predictors, but there is not straightforwards method to incorporate this information to a collaborative filtering model.   

Regarding the linear model, it yields a significantly weaker RMSE when compared to matrix factorization. Even so, a simple linear model which includes user and movie effects is enough to meet the exercise requirements. This model has the advantages of being straightforward to interpret and easy to implement. By using regularization techniques the model improves even further, but there is not a significant difference. Finally, the linear model could be improved by using the genre information and to some extend the timestamp information, but due to the memory limitations of this computer this could not be done.  

Both approaches yielded satisfactory results and met the required RMSE of the exercise being both valid methods to predict ratings. The linear model is computationally more efficient and is straightforward to understand an implement, but yields the worst RMSE, on the contrary, the collaborative filtering model is computationally demanding, and difficult to interpret, but with it we obtain the best RMSE.      


# Bibliography.

*Introduction to Data Science Data Analysis and Prediction Algorithms with R Rafael A. Irizarry 2021-07-03*

